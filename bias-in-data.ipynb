{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering Bias in Data\n",
    "\n",
    "The goal of this notebook is to explore the concept of bias in data using Wikipedia articles. This notebook will consider articles on political figures from different countries. We will combine a dataset of Wikipedia articles with a dataset of country populations, and use a machine learning service called ORES to estimate the quality of each article.\n",
    "We will then perform an analysis of how the coverage of politicians on Wikipedia and the quality of articles about politicians varies among countries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we start by importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the Data\n",
    "\n",
    "For this notebook we start with the two files in the raw-data folder which contain population and politician wikipedia data. We augment this with ORES quality predictions by doing a series of API pulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Page Info MediaWiki API \n",
    "The below cell accesses page info data using the [MediaWiki REST API for the EN Wikipedia](https://www.mediawiki.org/wiki/API:Main_page). The API documentation, [API:Info](https://www.mediawiki.org/wiki/API:Info), covers additional details that may be helpful when trying to use or understand this code.\n",
    "\n",
    "#### License\n",
    "The code below is built upon the example developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.2 - September 16, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page info for Batch:1\n",
      "Fetching page info for Batch:2\n",
      "Fetching page info for Batch:3\n",
      "Fetching page info for Batch:4\n",
      "Fetching page info for Batch:5\n",
      "Fetching page info for Batch:6\n",
      "Fetching page info for Batch:7\n",
      "Fetching page info for Batch:8\n",
      "Fetching page info for Batch:9\n",
      "Fetching page info for Batch:10\n",
      "Fetching page info for Batch:11\n",
      "Fetching page info for Batch:12\n",
      "Fetching page info for Batch:13\n",
      "Fetching page info for Batch:14\n",
      "Fetching page info for Batch:15\n",
      "Fetching page info for Batch:16\n",
      "Fetching page info for Batch:17\n",
      "Fetching page info for Batch:18\n",
      "Fetching page info for Batch:19\n",
      "Fetching page info for Batch:20\n",
      "Fetching page info for Batch:21\n",
      "Fetching page info for Batch:22\n",
      "Fetching page info for Batch:23\n",
      "Fetching page info for Batch:24\n",
      "Fetching page info for Batch:25\n",
      "Fetching page info for Batch:26\n",
      "Fetching page info for Batch:27\n",
      "Fetching page info for Batch:28\n",
      "Fetching page info for Batch:29\n",
      "Fetching page info for Batch:30\n",
      "Fetching page info for Batch:31\n",
      "Fetching page info for Batch:32\n",
      "Fetching page info for Batch:33\n",
      "Fetching page info for Batch:34\n",
      "Fetching page info for Batch:35\n",
      "Fetching page info for Batch:36\n",
      "Fetching page info for Batch:37\n",
      "Fetching page info for Batch:38\n",
      "Fetching page info for Batch:39\n",
      "Fetching page info for Batch:40\n",
      "Fetching page info for Batch:41\n",
      "Fetching page info for Batch:42\n",
      "Fetching page info for Batch:43\n",
      "Fetching page info for Batch:44\n",
      "Fetching page info for Batch:45\n",
      "Fetching page info for Batch:46\n",
      "Fetching page info for Batch:47\n",
      "Fetching page info for Batch:48\n",
      "Fetching page info for Batch:49\n",
      "Fetching page info for Batch:50\n",
      "Fetching page info for Batch:51\n",
      "Fetching page info for Batch:52\n",
      "Fetching page info for Batch:53\n",
      "Fetching page info for Batch:54\n",
      "Fetching page info for Batch:55\n",
      "Fetching page info for Batch:56\n",
      "Fetching page info for Batch:57\n",
      "Fetching page info for Batch:58\n",
      "Fetching page info for Batch:59\n",
      "Fetching page info for Batch:60\n",
      "Fetching page info for Batch:61\n",
      "Fetching page info for Batch:62\n",
      "Fetching page info for Batch:63\n",
      "Fetching page info for Batch:64\n",
      "Fetching page info for Batch:65\n",
      "Fetching page info for Batch:66\n",
      "Fetching page info for Batch:67\n",
      "Fetching page info for Batch:68\n",
      "Fetching page info for Batch:69\n",
      "Fetching page info for Batch:70\n",
      "Fetching page info for Batch:71\n",
      "Fetching page info for Batch:72\n",
      "Fetching page info for Batch:73\n",
      "Fetching page info for Batch:74\n",
      "Fetching page info for Batch:75\n",
      "Fetching page info for Batch:76\n",
      "Fetching page info for Batch:77\n",
      "Fetching page info for Batch:78\n",
      "Fetching page info for Batch:79\n",
      "Fetching page info for Batch:80\n",
      "Fetching page info for Batch:81\n",
      "Fetching page info for Batch:82\n",
      "Fetching page info for Batch:83\n",
      "Fetching page info for Batch:84\n",
      "Fetching page info for Batch:85\n",
      "Fetching page info for Batch:86\n",
      "Fetching page info for Batch:87\n",
      "Fetching page info for Batch:88\n",
      "Fetching page info for Batch:89\n",
      "Fetching page info for Batch:90\n",
      "Fetching page info for Batch:91\n",
      "Fetching page info for Batch:92\n",
      "Fetching page info for Batch:93\n",
      "Fetching page info for Batch:94\n",
      "Fetching page info for Batch:95\n",
      "Fetching page info for Batch:96\n",
      "Fetching page info for Batch:97\n",
      "Fetching page info for Batch:98\n",
      "Fetching page info for Batch:99\n",
      "Fetching page info for Batch:100\n",
      "Fetching page info for Batch:101\n",
      "Fetching page info for Batch:102\n",
      "Fetching page info for Batch:103\n",
      "Fetching page info for Batch:104\n",
      "Fetching page info for Batch:105\n",
      "Fetching page info for Batch:106\n",
      "Fetching page info for Batch:107\n",
      "Fetching page info for Batch:108\n",
      "Fetching page info for Batch:109\n",
      "Fetching page info for Batch:110\n",
      "Fetching page info for Batch:111\n",
      "Fetching page info for Batch:112\n",
      "Fetching page info for Batch:113\n",
      "Fetching page info for Batch:114\n",
      "Fetching page info for Batch:115\n",
      "Fetching page info for Batch:116\n",
      "Fetching page info for Batch:117\n",
      "Fetching page info for Batch:118\n",
      "Fetching page info for Batch:119\n",
      "Fetching page info for Batch:120\n",
      "Fetching page info for Batch:121\n",
      "Fetching page info for Batch:122\n",
      "Fetching page info for Batch:123\n",
      "Fetching page info for Batch:124\n",
      "Fetching page info for Batch:125\n",
      "Fetching page info for Batch:126\n",
      "Fetching page info for Batch:127\n",
      "Fetching page info for Batch:128\n",
      "Fetching page info for Batch:129\n",
      "Fetching page info for Batch:130\n",
      "Fetching page info for Batch:131\n",
      "Fetching page info for Batch:132\n",
      "Fetching page info for Batch:133\n",
      "Fetching page info for Batch:134\n",
      "Fetching page info for Batch:135\n",
      "Fetching page info for Batch:136\n",
      "Fetching page info for Batch:137\n",
      "Fetching page info for Batch:138\n",
      "Fetching page info for Batch:139\n",
      "Fetching page info for Batch:140\n",
      "Fetching page info for Batch:141\n",
      "Fetching page info for Batch:142\n",
      "Fetching page info for Batch:143\n",
      "Fetching page info for Batch:144\n",
      "Updated CSV saved as politicians_with_lastrevid.csv\n",
      "Failed page requests logged to failed_page_requests.log\n",
      "Titles with missing 'lastrevid' logged to missing_lastrevid.log\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CSV file with article titles (representing article IDs)\n",
    "csv_file_path = \"politicians_by_country_AUG.2024.csv\"\n",
    "politicians_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Set up Wikimedia API endpoint and request header\n",
    "API_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'asheera@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# Log file to store failed article titles and those with missing 'lastrevid'\n",
    "failed_page_requests_log = \"failed_page_requests.log\"\n",
    "missing_lastrevid_log = \"missing_lastrevid.log\"\n",
    "\n",
    "# Function to log failed requests\n",
    "def log_failed_titles(titles, log_file_path):\n",
    "    with open(log_file_path, \"a\") as log_file:  # Open in append mode\n",
    "        log_file.write(\"\\n\".join(titles) + \"\\n\")\n",
    "\n",
    "# Function to make batch requests for a group of article titles\n",
    "def fetch_page_info(titles):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"info\",\n",
    "        \"titles\": \"|\".join(titles),\n",
    "        \"inprop\": \"url\"\n",
    "    }\n",
    "    response = requests.get(API_ENDPOINT, headers=HEADERS, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('query', {}).get('pages', {})\n",
    "    else:\n",
    "        # Log the failed titles\n",
    "        log_failed_titles(titles, failed_page_requests_log)\n",
    "        return {}\n",
    "\n",
    "# Split article names into batches of 50 for API requests\n",
    "batch_size = 50\n",
    "article_batches = [politicians_df['name'][i:i + batch_size].tolist() for i in range(0, len(politicians_df), batch_size)]\n",
    "\n",
    "# Dictionary to store lastrevid for each article\n",
    "lastrevid_dict = {}\n",
    "\n",
    "i = 1\n",
    "# Loop through each batch and make API requests\n",
    "for batch in article_batches:\n",
    "    print(f\"Fetching page info for Batch:{i}\")\n",
    "    i+=1\n",
    "    page_info = fetch_page_info(batch)\n",
    "    \n",
    "    # Extract lastrevid for each article and store in the dictionary\n",
    "    for page_id, page_data in page_info.items():\n",
    "        title = page_data['title']\n",
    "        lastrevid = page_data.get('lastrevid', None)\n",
    "        \n",
    "        if lastrevid is not None:\n",
    "            lastrevid_dict[title] = lastrevid\n",
    "        else:\n",
    "            # Log the title with missing 'lastrevid'\n",
    "            log_failed_titles([title], missing_lastrevid_log)\n",
    "\n",
    "    # Respect API limits with a small delay\n",
    "    time.sleep(API_THROTTLE_WAIT)  \n",
    "\n",
    "# Add lastrevid as a new column to the DataFrame\n",
    "politicians_df['lastrevid'] = politicians_df['name'].map(lastrevid_dict)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_csv_path = \"politicians_with_lastrevid.csv\"\n",
    "politicians_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV saved as {output_csv_path}\")\n",
    "print(f\"Failed page requests logged to {failed_page_requests_log}\")\n",
    "print(f\"Titles with missing 'lastrevid' logged to {missing_lastrevid_log}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting ORES scores through LiftWing ML Service API\n",
    "\n",
    "The cell below generates article quality estimates for article revisions using the LiftWing version of [ORES](https://www.mediawiki.org/wiki/ORES). The [ORES API documentation](https://ores.wikimedia.org) can be accessed from the main ORES page.\n",
    "\n",
    "You will need a Wikimedia user account to get access to Lift Wing (the ML API service). You can either [create an account or login](https://api.wikimedia.org/w/index.php?title=Special:UserLogin). This notebook used the option to create a [Personal API token](https://api.wikimedia.org/wiki/Authentication) and stores it in a .env file. \n",
    "\n",
    "#### License\n",
    "The code below builds upon the example developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.0 - August 15, 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "#\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = ((60.0*60.0)/5000.0)-API_LATENCY_ASSUMED  # The key authorizes 5000 requests per hour\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<{email_address}>, University of Washington, MSDS DATA 512 - AUTUMN 2024\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "#\n",
    "ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "USERNAME = \"Apoorvasheera\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the personal access token form the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Loads the .env file\n",
    "\n",
    "CLIENT_ID = os.getenv('CLIENT_ID')\n",
    "CLIENT_SECRET = os.getenv('CLIENT_SECRET')\n",
    "ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to encapsulate the API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of the function and output format from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting LiftWing ORES scores for 'Bison' with revid: 1085687913\n",
      "{\n",
      "    \"enwiki\": {\n",
      "        \"models\": {\n",
      "            \"articlequality\": {\n",
      "                \"version\": \"0.9.2\"\n",
      "            }\n",
      "        },\n",
      "        \"scores\": {\n",
      "            \"1085687913\": {\n",
      "                \"articlequality\": {\n",
      "                    \"score\": {\n",
      "                        \"prediction\": \"FA\",\n",
      "                        \"probability\": {\n",
      "                            \"B\": 0.07895665991827401,\n",
      "                            \"C\": 0.03728215742560417,\n",
      "                            \"FA\": 0.5629436065906797,\n",
      "                            \"GA\": 0.30547854835374505,\n",
      "                            \"Start\": 0.011061807252218824,\n",
      "                            \"Stub\": 0.00427722045947826\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#   \n",
    "#\n",
    "#   Which article - the key for the article dictionary defined above\n",
    "article_title = \"Bison\"\n",
    "#\n",
    "print(f\"Getting LiftWing ORES scores for '{article_title}' with revid: {ARTICLE_REVISIONS[article_title]:d}\")\n",
    "#\n",
    "#    Make the call, just pass in the article revision ID, email address, and access token\n",
    "score = request_ores_score_per_article(article_revid= ARTICLE_REVISIONS[article_title],\n",
    "                                       email_address=\"apoorvasheera98@gmail.com\",\n",
    "                                       access_token=ACCESS_TOKEN)\n",
    "#\n",
    "#    Output the result\n",
    "print(json.dumps(score,indent=4))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving ORES Quality Predictions for Wikipedia Articles\n",
    "\n",
    "In this step, we load a CSV file containing Wikipedia articles about politicians along with their most recent revision IDs. We check if a partially completed predictions file exists (`politicians_with_partial_prediction.csv`) and resume from the last processed index. If not, we start from the beginning, initializing an empty predictions list.\n",
    "\n",
    "To retrieve the quality predictions, we use the ORES API, which requires the revision ID and article title. The API is rate-limited, so we introduce a delay between requests to avoid exceeding limits. We log failed requests and errors to a separate log file (`ores_request_errors.log`) for review.\n",
    "\n",
    "The main functionality includes:\n",
    "- Loading the access token from an environment variable for API authentication.\n",
    "- Defining helper functions for logging failed requests and extracting predictions from the API response.\n",
    "- Iterating through each article and making requests to the ORES API to get quality predictions.\n",
    "- After each request, we save the updated DataFrame to a CSV file (`politicians_with_partial_prediction.csv`) to ensure progress is saved.\n",
    "\n",
    "The process resumes from the last successfully completed request, ensuring the program is resilient to failures and can continue without losing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting LiftWing ORES scores for 'Ajok Lucy' with revid: 1236299635\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jack Odur Lutanywa' with revid: 1141688429\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'James Mamawi' with revid: 1246243211\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Margaret Rwebyambu' with revid: 1246348833\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Margret Okunga Makoha' with revid: 1246243219\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jack Maumbe Mukhwana' with revid: 1246244527\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Willy Mayambala' with revid: 1249062770\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Kibirige Mayanja' with revid: 1108117021\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Tamale Mirundi' with revid: 1242742635\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Milly Mugeni' with revid: 1164574518\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Isaac Mulindwa' with revid: 1246242602\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Besueri Kiwanuka Lusse Mulondo' with revid: 1061210640\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Victoria Mwaka' with revid: 1152489436\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Fred Mwesigye' with revid: 1246243407\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mary Annet Nakato' with revid: 1239422712\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Faridah Nambi' with revid: 1161962008\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ndyomugyenyi Roland Bish' with revid: 1235383312\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Linos Ngompek' with revid: 1234362908\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Asinansi Nyakato' with revid: 1239308674\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Geoffrey Onegi Obel' with revid: 1126114101\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Onegi Obel' with revid: 1102430835\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'James Obita' with revid: 994193575\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Apollo Ofwono' with revid: 1246243537\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Amos Okot' with revid: 1246243558\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Moses Okot Jr' with revid: 1247475870\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Okullo Aabuka Jallon Anthony' with revid: 1234691084\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Bosmic Otim' with revid: 1240241204\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Patrick Nsamba Oshabe' with revid: 1235970119\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ricky Richard Anywar' with revid: 1246243589\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Flavia Rwabuhoro Kabahenda' with revid: 1235319704\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Eric Sakwa' with revid: 1092463612\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sitenda Sebalu' with revid: 1231448473\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mayanja Kyompitira Sebalu' with revid: 1217885513\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Simeo Muwanga Nsubuga' with revid: 1246243626\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Wilson Tumwine' with revid: 1142485232\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sarah Nambozo Wekomba' with revid: 1225047761\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sergey Abisov' with revid: 1236963594\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Yakiv Apter' with revid: 1248793136\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Volodymyr Artyukh' with revid: 1251017521\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Alexander Barybin' with revid: 1163042932\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Olha Bench' with revid: 1242417477\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Serhiy Berezenko' with revid: 1229476867\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Andriy Beshta' with revid: 1210899128\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ivan Bisyuk' with revid: 1249219732\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Volodymyr Boyko' with revid: 1239786642\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ihor Dolhov' with revid: 1246746604\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Anatoliy Fedorchuk' with revid: 1180336074\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ivan Gel' with revid: 1221633541\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Moshe Gutman' with revid: 1226775102\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Olena Halushka' with revid: 1233249695\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Izet Hdanov' with revid: 1246833115\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Artur Herasymov' with revid: 1250198673\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Valeriy Holovko' with revid: 1248663108\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Volodymyr Hryshchenko' with revid: 1248793385\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Stepan Ivakhiv' with revid: 1210915594\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Borys Kachura' with revid: 1243514456\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Dmytro Kashchuk' with revid: 1249706357\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Serhii Khlan' with revid: 1225922449\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Borys Klimchuk (politician)' with revid: 1243514473\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Klitschko brothers' with revid: 1247700818\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mykola Kolisnyk' with revid: 1241708390\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mykhailo Korolenko' with revid: 1245463505\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ivan Korshynskyi' with revid: 1246362674\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Roman Kostenko' with revid: 1246289591\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Heorhiy Kryuchkov' with revid: 1246363667\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mykola Kulinich' with revid: 1243512952\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Vitalii Kurylo' with revid: 1179344499\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Anatolii Kutsevol' with revid: 1213685875\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ihor Kyzym' with revid: 1198046414\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Hennadiy Lahuta' with revid: 1248754715\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Yurii Lavreniuk' with revid: 1204238271\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Andriy Levus' with revid: 1208846461\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ivan Lypa' with revid: 1158193136\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Roman Mashovets' with revid: 1235987628\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Denys Maslov' with revid: 1239094096\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Volodymyr Matvienko' with revid: 1230557950\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Pavlo Matvienko' with revid: 1246362611\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Oleh Meidych' with revid: 1177396640\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mykola Popov' with revid: 1203519635\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Denys Myrgorodskyi' with revid: 1251114265\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sergey Nazarov' with revid: 1238945724\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Andriy Odarchenko' with revid: 1247477202\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Hryhoriy Omelchenko' with revid: 1164218357\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Volodymyr Petrov' with revid: 1244845824\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mykola Petruk' with revid: 1248656754\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Vasyl Poraiko' with revid: 1213107063\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Yelena Protsenko' with revid: 1243514438\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mykhailo Reznik' with revid: 1198044363\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Oleksandr Ryzhenkov' with revid: 1217933060\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Petro Gadz' with revid: 1242152454\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ruslan Magomedov (economist)' with revid: 1214631880\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Oleksandr Ruzhytskyi' with revid: 1243514499\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mykola Riabovil' with revid: 1247346956\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Vladimir Saldo' with revid: 1250298214\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Olena Semeniaka' with revid: 1220262481\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Volodymyr Serhiychuk' with revid: 1179345129\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Oleh Shamshur' with revid: 1245069798\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Vyacheslav Shapovalov' with revid: 1183782262\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Serhii Shapran' with revid: 1249146625\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Iryna Stavchuk' with revid: 1237135024\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Volodymyr Stelmakh' with revid: 1249875856\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Viacheslav Suprunenko' with revid: 1146431478\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Taras Tarasenko' with revid: 1194398452\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Volodymyr Tsvil' with revid: 1232601928\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Volodymyr Tyahlo' with revid: 1179346831\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Serhiy Tymchenko' with revid: 1249574861\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Kyrylo Tymoshenko' with revid: 1243513166\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Evgeniy Udod' with revid: 1246967830\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Dmytro Vorona' with revid: 1243566650\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mykola Yankovsky' with revid: 1182811623\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Kostiantyn Yastrub' with revid: 1187254577\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Yuriy Vozniuk' with revid: 1179848407\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Andriy Yusov' with revid: 1213377592\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Anatoliy Zasukha' with revid: 1248186807\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Vitaly Zholobov' with revid: 1243513404\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Luis Almagro' with revid: 1246000343\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sebastián Bauzá' with revid: 1237709578\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mónica Bottero' with revid: 1158316569\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jorge Bruni' with revid: 1101182986\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Guillermo Chifflet' with revid: 1140500454\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Laura Cortinas' with revid: 1241843549\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Guillermo Domenech' with revid: 1236521270\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Lorenzo Antonio Fernández' with revid: 1225789614\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Pedro Figari' with revid: 1250777073\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Liliam Kechichián' with revid: 1239924729\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Dámaso Antonio Larrañaga' with revid: 1209913980\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Juan Francisco Larrobla' with revid: 1159863480\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Eduardo Lorier' with revid: 1238776852\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Hector Luisi' with revid: 1192169906\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Guido Manini Ríos' with revid: 1235341731\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Isidoro de María' with revid: 1248513982\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Luis Melián Lafinur' with revid: 946357029\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Antonio Mercader' with revid: 1193060468\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Pablo Mieres' with revid: 1234857670\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'José Benito Monterroso' with revid: 1225811185\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Tristán Narvaja' with revid: 1206959075\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Gonzalo Nin Novoa' with revid: 1106146731\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Edgardo Novick' with revid: 1232110434\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'María Inés Obaldía' with revid: 1227581396\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Yamandú Orsi' with revid: 1251015262\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'José Brito del Pino' with revid: 1231236699\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Enrique Pintado' with revid: 1229665135\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Carlos Pita (politician)' with revid: 1225819287\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Carlos Quijano' with revid: 1172801438\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Laura Raffo' with revid: 1232440725\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'José Rondeau' with revid: 1229765194\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Florencio Sánchez' with revid: 1244429745\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'José Pedro Varela' with revid: 1184349404\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Pedro Vaz (diplomat)' with revid: 1108530199\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mateo Vidal' with revid: 1093592713\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jorge Zabalza' with revid: 1232110666\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ikhtiyor Abdullayev' with revid: 1247702597\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Adham Ahmedbaev' with revid: 1247704434\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jakhongir Artikkhodjayev' with revid: 1230521779\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Valery Ataev' with revid: 1205273470\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Abdusalom Azizov' with revid: 1246468761\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Uktam Barnoev' with revid: 1224037855\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sa'dulla Begaliyev' with revid: 1247704310\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Torokul Dzhanuzakov' with revid: 1242155530\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Rustam Inoyatov' with revid: 1247702441\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Rashid Kadyrov' with revid: 1247702823\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Karim Kamalov' with revid: 1247703243\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Tatyana Karimova' with revid: 1245927956\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Erkin Khalilov' with revid: 1242468179\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sherzodkhon Kudratkhuja' with revid: 1214067513\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Bakhodir Kurbanov (general)' with revid: 1220771041\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jahangir Mamatov' with revid: 1158281467\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Rimajon Xudoyberganova' with revid: 1242391949\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Boʻritosh Mustafoyev' with revid: 1157984320\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Bahrom Norqobilov' with revid: 1219558071\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Narzullo Oblomurodov' with revid: 1247703142\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ravshanbek Qurbonov' with revid: 1248998037\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Baxtiyor Rahimov' with revid: 1248651514\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Bekjon Rakhmonov' with revid: 1226100587\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Bakhodir Khan Turkistan' with revid: 1192709223\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Qobul Tursunov' with revid: 1213407495\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Vincent Lunabek' with revid: 1132674478\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'President of Vanuatu' with revid: 1211211712\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jimmy Stevens (politician)' with revid: 1169502061\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Dinh Van Than' with revid: 1244170639\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Fernando Adames Torres' with revid: 1217851956\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ivonne Attas' with revid: 1250286831\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Fidel Barbarito' with revid: 1246246398\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Pedro Bastidas' with revid: 1232135029\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Alejandra Benítez' with revid: 1246651957\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'José Brito (politician)' with revid: 1246246718\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Buenaventura Macabeo Maldonado Vivas' with revid: 1231707762\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Rubén González Cárdenas' with revid: 1240178755\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'José Curiel' with revid: 1226437176\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Felix Díaz Ortega' with revid: 1202899562\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Vicente José Gregorio Díaz Silva' with revid: 1242413774\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Alí Domínguez' with revid: 1240451011\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Franklyn Duarte' with revid: 1246246730\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Alberto Franceschi' with revid: 1198782364\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Arnoldo Gabaldón' with revid: 1216357280\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Vladimir Gessen (Venezuelan politician)' with revid: 1225792818\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jorge Giordani' with revid: 1246246427\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'César González Martínez' with revid: 1218574777\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Luis Graterol Caraballo' with revid: 1248971751\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Liborio Guarulla' with revid: 1242552557\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'José Bernabé Gutiérrez' with revid: 1218120878\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'María Cristina Iglesias' with revid: 1246246447\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Reinaldo Iturriza' with revid: 1246246455\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Andrés Izarra' with revid: 1246246457\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Reinaldo Leandro Mora' with revid: 1195139267\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Carlos Augusto León' with revid: 1232115163\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ramón Lobo (economist)' with revid: 1246246763\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jorge Elieser Márquez' with revid: 1219284052\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Américo Martín' with revid: 1246354258\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nelson Martínez (politician)' with revid: 1246246463\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Francisco de Miranda' with revid: 1247134615\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Reinaldo Muñoz Pedroza' with revid: 1189619482\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Dia Nader de El-Andari' with revid: 1243220418\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Moisés Naím' with revid: 1247919584\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Freddy Ñáñez' with revid: 1246246481\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Pablo Acosta Ortiz' with revid: 1243754078\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Renny Ottolina' with revid: 1241457122\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Alejandro Peña Esclusa' with revid: 1247050566\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Miguel Peña (politician)' with revid: 1228343895\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Lewis Pérez' with revid: 1232134166\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Arkiely Perfecto' with revid: 1246246976\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Eduardo Piñate' with revid: 1246246484\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Humberto Prado' with revid: 1097454386\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Omar Prieto' with revid: 1232134990\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Benjamín Rausseo' with revid: 1246857158\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Manuel Felipe Rugeles' with revid: 1208177120\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Carlos Santana Tovar' with revid: 1242453867\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Milena Sardi de Selle' with revid: 1145661230\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Carlos Eduardo Stolk' with revid: 1245089053\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Bolivia Suárez' with revid: 1246247008\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Gustavo Tarre' with revid: 1245587280\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'José Herrera Uslar' with revid: 1153086780\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Addy Valero' with revid: 1246247019\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'José Antonio Velutini' with revid: 1135427671\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Henry Ventura' with revid: 1246246808\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nervis Villalobos' with revid: 1201838885\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Bùi Tiến Dũng (politician)' with revid: 1209293575\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Chu Văn An' with revid: 1244809536\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Quốc Định' with revid: 1221993606\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Đinh Xuân Quảng' with revid: 1238293364\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Kiem Do' with revid: 1238520532\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Đỗ Quang Giai' with revid: 1229375855\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Hồ Đắc Điềm' with revid: 1193882673\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Hồ Văn Nhựt' with revid: 1238255201\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Hoàng Cao Khải' with revid: 1241426352\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Hoang Van Chi' with revid: 1249314533\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Huỳnh Văn Cao' with revid: 1233300105\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Kiều Mộng Thu' with revid: 1242490895\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Lê Minh Hương' with revid: 1226261855\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Lê Quốc Minh' with revid: 1241039136\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Lê Văn Thành' with revid: 1226261978\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Lương Thế Huy' with revid: 1247377913\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Hồng Diên' with revid: 1237710183\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Đức Chung' with revid: 1248452284\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Dương Đôn' with revid: 1177240282\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Duy Quý' with revid: 1245479331\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Hợp Đoàn' with revid: 1232872822\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Phan Long' with revid: 1221992596\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Quốc Cường' with revid: 1130925190\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Thị Hồng (economist)' with revid: 1214023850\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Thị Thập' with revid: 1145703738\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Văn Huyền' with revid: 1241609893\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Văn Lực' with revid: 1250942954\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Văn Trân' with revid: 1245671291\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nguyễn Văn Xuân' with revid: 1231154121\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Phan Anh' with revid: 1224595790\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Tạ Quang Bửu' with revid: 1224595798\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Tran Minh Tiet' with revid: 1225066058\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Trần Quốc Vượng (politician)' with revid: 1237490552\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Trần Văn Hữu' with revid: 1244544286\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Trương Đình Dzu' with revid: 1220804010\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Võ Văn Thưởng' with revid: 1241845313\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Abdullah Noman' with revid: 1248329006\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Hashid Abdullah al-Ahmar' with revid: 1247681421\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Himyar Abdullah al-Ahmar' with revid: 1247681426\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sam Yahya Al-Ahmar' with revid: 1247681434\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ameen Ali al-Akimi' with revid: 1139612517\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Talal Aklan' with revid: 1242218278\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Abdullatif Al-Sayed' with revid: 1231304528\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Muhammad Musa al-Amri' with revid: 1144463258\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mohamed al-Atifi' with revid: 1250951500\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Yasser Al-Awadi' with revid: 1129096674\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sagheer Hamoud Aziz' with revid: 1247681445\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sultan al-Barakani' with revid: 1246507826\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Faraj Salmin Al-Bahsani' with revid: 1216872257\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ali Al Bukhaiti' with revid: 1249864801\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Bruce Conde' with revid: 1243942923\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Abdul Wahab Al-Dailami' with revid: 1189134411\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ali Ahmad Nasser al-Dhahab' with revid: 1106368440\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Badreddin al-Houthi' with revid: 1248072502\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ibrahim Bin Ali Al Wazeer' with revid: 1144463479\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mohammed Lutf al-Iryani' with revid: 1219000221\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Abd Al-Rahman Ali Al-Jifri' with revid: 1144463695\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Hussein Khairan' with revid: 1242390966\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mohammed Ali Al-Maqdashi' with revid: 1228062555\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Hooria Mashhour' with revid: 1241264144\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Muhammad bin Yahya bin Muhammad' with revid: 1231305316\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Abdul Qader Qahtan' with revid: 1224446773\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Abdulwahab al-Rawhani' with revid: 1237331344\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mohammed al-Sabry' with revid: 1232803611\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Yahya Mohamed Abdullah Saleh' with revid: 1223542345\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ali Taysir' with revid: 1179127922\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Yahya Muhammad Hamid ed-Din' with revid: 1249145023\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Aidarus al-Zoubaidi' with revid: 1243417460\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Francis Kapyanga' with revid: 1241327476\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Lily Monze' with revid: 1245511378\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Olipa Phiri' with revid: 1180049589\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Gift Banda' with revid: 1227050005\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Flora Buka' with revid: 1235342444\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Bulelani Lobengula' with revid: 1240505790\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Joseph Makamba Busha' with revid: 1215996308\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Darlington Dzikamai Chigumbu' with revid: 1217727776\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'James Chikerema' with revid: 1247967827\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Paul Chimedza' with revid: 1226620788\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Chenhamo Chimutengwende' with revid: 1246239471\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ruth Chinamano' with revid: 1233377754\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'George Chiweshe' with revid: 1237333574\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Phillip Chiyangwa' with revid: 1204162245\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Guy Clutton-Brock' with revid: 1233375199\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Margaret Dongo' with revid: 1201585392\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jabulani Dube' with revid: 1084850581\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Fletcher Dulini Ncube' with revid: 1236276553\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ken Flower' with revid: 1246135863\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Aguy Georgias' with revid: 1233377690\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sithembile Gumbo' with revid: 1246239507\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Josiah Zion Gumede' with revid: 1230504811\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Ellen Gwaradzimba' with revid: 1246241932\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Andre Sothern Holland' with revid: 1203015285\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Byron Hove' with revid: 1226322417\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Tonderai Kasu' with revid: 1231961724\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Wilson Khumbula' with revid: 1246239559\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Lovemore Madhuku' with revid: 1138179170\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sarah Mahoka' with revid: 1246239576\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Alfred Makwarimba' with revid: 1092395186\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Washington Malianga' with revid: 1064532257\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Elliot Manyika' with revid: 1246001588\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Linda Masarira' with revid: 1177290618\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jevan Maseko' with revid: 1084850620\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Anxious Masuka' with revid: 1247106176\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Lovemore Matombo' with revid: 1105192324\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Bright Matonga' with revid: 1036801272\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Isaac Matongo' with revid: 1246276309\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Obedingwa Mguni' with revid: 1247645001\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Promise Mkwananzi' with revid: 1246747822\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Max Mnkandla' with revid: 1007344798\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Bekithemba Mpofu' with revid: 1085044064\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Earnest Mudzengi' with revid: 1246271197\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Mugabe family' with revid: 1245786617\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Kisinoti Mukwazhe' with revid: 1108499694\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Edwin Mushoriwa' with revid: 1246239657\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Munacho Mutezo' with revid: 1112725980\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Abel Muzorewa' with revid: 1246262690\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Prag Lalloo Naran' with revid: 1170189291\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Owen Ncube' with revid: 1246242012\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nhlanhlayamangwe Felix Ndiweni' with revid: 1193170496\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Gundwane Ndiweni' with revid: 1223100611\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Khayisa Ndiweni' with revid: 1226322430\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Jane Ngwenya' with revid: 1199625809\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Maurice Nyagumbo' with revid: 1225102400\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'George Nyandoro' with revid: 1143871016\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sekai Nzenza' with revid: 1241843269\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Engelbert Rugeje' with revid: 1250829674\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Tinos Rusere' with revid: 1236282175\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Scot Sakupwanya' with revid: 1242611251\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Daniel Shumba' with revid: 1154573139\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Maxwell Zeb Shumba' with revid: 1246242430\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Nkululeko Mkastos Sibanda' with revid: 1243016866\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Leopold Takawira' with revid: 1236900271\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Gift Tandare' with revid: 1235705451\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Rekayi Tangwena' with revid: 1236117496\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Temba Mliswa' with revid: 1246241769\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Josiah Tongogara' with revid: 1203429435\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Langton Towungana' with revid: 1246280093\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Sengezo Tshabangu' with revid: 1228478288\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Herbert Ushewokunze' with revid: 959111842\n",
      "Success\n",
      "Getting LiftWing ORES scores for 'Denis Walker' with revid: 1247902630\n",
      "Success\n",
      "Updated CSV saved as politicians_with_partial_prediction.csv\n",
      "Failed requests logged to ores_request_errors.log\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_file_path = \"politicians_with_lastrevid.csv\"\n",
    "politicians_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Check if you already have some predictions and resume from there\n",
    "output_csv_path = \"politicians_with_partial_prediction.csv\"\n",
    "if os.path.exists(output_csv_path):\n",
    "    politicians_df = pd.read_csv(output_csv_path)\n",
    "    predictions = politicians_df['prediction'].tolist()  # Load existing predictions\n",
    "    start_index = 6779 # Manually restarting from where previous request failed\n",
    "else:\n",
    "    predictions = [None] * len(politicians_df)  # Initialize empty predictions\n",
    "    start_index = 0  # Start from the beginning if no previous results\n",
    "\n",
    "# Log file to store failed article titles or errors\n",
    "log_file_path = \"ores_request_errors.log\"\n",
    "\n",
    "# ORES API constants\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "API_LATENCY_ASSUMED = 0.002\n",
    "API_THROTTLE_WAIT = ((60.0*60.0)/5000.0) - API_LATENCY_ASSUMED\n",
    "\n",
    "# Load access token and other environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')\n",
    "\n",
    "# Function to log failed requests\n",
    "def log_failed_request(title, error_message):\n",
    "    with open(log_file_path, \"a\") as log_file:\n",
    "        log_file.write(f\"{title}: {error_message}\\n\")\n",
    "\n",
    "# Function to make ORES request for each article\n",
    "def request_ores_score_per_article(article_title, article_revid, email_address, access_token):\n",
    "    # Define request data and header\n",
    "    request_data = {\n",
    "        \"lang\": \"en\",\n",
    "        \"rev_id\": article_revid,\n",
    "        \"features\": True\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': f\"{email_address}, University of Washington, MSDS DATA 512 - AUTUMN 2024\",\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f\"Bearer {access_token}\"\n",
    "    }\n",
    "    \n",
    "    request_url = API_ORES_LIFTWING_ENDPOINT.format(model_name=API_ORES_EN_QUALITY_MODEL)\n",
    "    \n",
    "    try:\n",
    "        time.sleep(API_THROTTLE_WAIT)  # Throttle to avoid hitting rate limits\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        json_response = response.json()\n",
    "        return json_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_failed_request(article_title, str(e))\n",
    "        return None\n",
    "\n",
    "# Function to extract prediction from the ORES API response\n",
    "def extract_prediction(json_response,article_revid):\n",
    "    try:\n",
    "        prediction = json_response['enwiki']['scores'][str(article_revid)][\"articlequality\"][\"score\"][\"prediction\"]\n",
    "        return prediction\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Loop through each article and make ORES requests\n",
    "# Resume from the next index after the last completed request\n",
    "for index, row in politicians_df.iloc[start_index:].iterrows():\n",
    "    article_title = row['name']\n",
    "    article_revid = row['lastrevid']\n",
    "    \n",
    "    if pd.notnull(article_revid):  # Ensure revid exists\n",
    "        article_revid = int(article_revid)\n",
    "        print(f\"Getting LiftWing ORES scores for '{article_title}' with revid: {article_revid}\")\n",
    "        json_response = request_ores_score_per_article(article_title, article_revid, \"apoorvasheera98@gmail.com\", ACCESS_TOKEN)\n",
    "        \n",
    "        if json_response:\n",
    "            print(\"Success\")\n",
    "            prediction = extract_prediction(json_response, article_revid)\n",
    "            if prediction:\n",
    "                predictions[index] = prediction\n",
    "            else:\n",
    "                log_failed_request(article_title, \"Prediction missing in ORES response\")\n",
    "                predictions[index] = None\n",
    "        else:\n",
    "            print(\"Failure\")\n",
    "            predictions[index] = None\n",
    "    else:\n",
    "        log_failed_request(article_title, \"Missing revid\")\n",
    "        predictions[index] = None\n",
    "\n",
    "    # Save the updated DataFrame with predictions after each request\n",
    "    politicians_df['prediction'] = predictions\n",
    "    politicians_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"Updated CSV saved as {output_csv_path}\")\n",
    "print(f\"Failed requests logged to {log_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Timeout Errors and Retrying ORES Requests\n",
    "\n",
    "In this cell, we handle the cases where previous ORES requests failed due to a 504 Gateway Timeout error. The goal is to identify these failed requests, retry them, and update the predictions in our dataset.\n",
    "\n",
    "The process is broken into two functions:\n",
    "1. **get_titles_with_timeout_errors:**  \n",
    "   This function reads through the error log file (`ores_request_errors.log`) and extracts the titles of articles that encountered a 504 error during the ORES request. These titles are returned as a list, allowing us to retry the requests for just these problematic articles.\n",
    "   \n",
    "2. **retry_ores_requests_for_timeout_titles:**  \n",
    "   This function takes the list of titles with timeout errors and retries the ORES API request for each one. It locates the relevant article in the dataset, makes the ORES request again, and updates the prediction if a valid response is received. After processing all the retries, the updated DataFrame is saved back to the CSV file (`politicians_with_partial_prediction.csv`).\n",
    "\n",
    "By retrying only the failed requests, we ensure that the entire dataset is updated without reprocessing articles that were already completed successfully. This approach improves efficiency and ensures a complete dataset for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying ORES request for 'André Resampa' with revid: 1191255852\n",
      "Updated prediction for 'André Resampa'\n",
      "Retrying ORES request for 'Malik Allahyar Khan' with revid: 1233679331\n",
      "Updated prediction for 'Malik Allahyar Khan'\n",
      "Retrying ORES request for 'József Klekl (politician)' with revid: 1185528975\n",
      "Updated prediction for 'József Klekl (politician)'\n",
      "Retrying ORES request for 'Jožef Krajnc' with revid: 1239252872\n",
      "Updated prediction for 'Jožef Krajnc'\n",
      "Retrying ORES request for 'Matija Majar' with revid: 1195006555\n",
      "Updated prediction for 'Matija Majar'\n",
      "Retrying ORES request for 'Aziz Feyzi Pirinççizâde' with revid: 1240060888\n",
      "Updated prediction for 'Aziz Feyzi Pirinççizâde'\n",
      "Retrying ORES request for 'Mammetmyrat Geldinyyazov' with revid: 1138728249\n",
      "Updated prediction for 'Mammetmyrat Geldinyyazov'\n",
      "Retrying ORES request for 'Karubanga Jacob' with revid: 1234106478\n",
      "Updated prediction for 'Karubanga Jacob'\n",
      "Updated CSV saved as politicians_with_partial_prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the log file and extract titles with the 504 Gateway Timeout error\n",
    "def get_titles_with_timeout_errors(log_file_path):\n",
    "    titles_with_timeout = []\n",
    "    with open(log_file_path, \"r\") as log_file:\n",
    "        for line in log_file:\n",
    "            if \"504 Server Error\" in line:\n",
    "                title = line.split(\":\")[0]  # Extract the title before the colon\n",
    "                titles_with_timeout.append(title.strip())\n",
    "    return titles_with_timeout\n",
    "\n",
    "# Retry the ORES requests for the titles with 504 errors\n",
    "def retry_ores_requests_for_timeout_titles(titles_with_timeout, csv_file_path):\n",
    "    # Load the existing CSV with predictions\n",
    "    politicians_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Loop through the titles and retry requests\n",
    "    for title in titles_with_timeout:\n",
    "        row = politicians_df[politicians_df['name'] == title]\n",
    "        if not row.empty:\n",
    "            article_revid = row['lastrevid'].values[0]\n",
    "            if pd.notnull(article_revid):  # Ensure revid exists\n",
    "                article_revid = int(article_revid)\n",
    "                print(f\"Retrying ORES request for '{title}' with revid: {article_revid}\")\n",
    "                json_response = request_ores_score_per_article(title, article_revid, \"apoorvasheera98@gmail.com\", ACCESS_TOKEN)\n",
    "                \n",
    "                if json_response:\n",
    "                    prediction = extract_prediction(json_response, article_revid)\n",
    "                    if prediction:\n",
    "                        # Update the prediction in the DataFrame\n",
    "                        politicians_df.loc[politicians_df['name'] == title, 'prediction'] = prediction\n",
    "                        print(f\"Updated prediction for '{title}'\")\n",
    "                    else:\n",
    "                        print(f\"Prediction missing in ORES response for '{title}'\")\n",
    "    \n",
    "    # Save the updated DataFrame\n",
    "    politicians_df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Updated CSV saved as {csv_file_path}\")\n",
    "\n",
    "# Path to the log file and CSV file\n",
    "log_file_path = \"ores_request_errors.log\"\n",
    "csv_file_path = \"politicians_with_partial_prediction.csv\"\n",
    "\n",
    "# Get the list of titles with 504 errors\n",
    "titles_with_timeout_errors = get_titles_with_timeout_errors(log_file_path)\n",
    "\n",
    "# Retry requests and update the predictions\n",
    "retry_ores_requests_for_timeout_titles(titles_with_timeout_errors, csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structuring the Population Data with Regions\n",
    "\n",
    "We process the population dataset (`population_by_country_AUG.2024.csv`) to add a column for geographic regions. The original dataset includes both countries and regions (e.g., continents or groups of countries), but it does not explicitly associate each country with its region. This restructuring will allow us to later analyze Wikipedia article coverage and quality on both a country-by-country and regional basis.\n",
    "\n",
    "Key steps in this process:\n",
    "1. **Remove irrelevant rows:**  \n",
    "   The row for 'WORLD' is manually removed as it's not useful for our analysis.\n",
    "   \n",
    "2. **Detect countries vs. regions:**  \n",
    "   We use a function to identify whether a row represents a country or a region by checking if the 'Geography' value is in uppercase. Rows with uppercase 'Geography' values represent regions (e.g., \"AFRICA\"), while lowercase values represent countries.\n",
    "\n",
    "3. **Assign regions to countries:**  \n",
    "   We iterate through the dataset and, for each country, assign it to the most recent region (from the uppercase rows). This helps ensure that each country is associated with its respective region.\n",
    "\n",
    "4. **Save the updated data:**  \n",
    "   The resulting DataFrame, with the new 'region' column, is saved as `population_by_country_with_regions.csv`, which will be used in subsequent analysis steps.\n",
    "\n",
    "This restructuring is essential for our analysis of Wikipedia articles at both country and regional levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated population data with regions saved as intermediate-data/population_by_country_with_regions.csv\n"
     ]
    }
   ],
   "source": [
    "# Converting the population file to a structured format (adding columns\n",
    "# with region and continent)\n",
    "# Load the population_df\n",
    "population_df = pd.read_csv(\"raw-data/population_by_country_AUG.2024.csv\")\n",
    "\n",
    "# Remove the 'WORLD' row manually since it's not relevant\n",
    "population_df = population_df[population_df['Geography'] != 'WORLD']\n",
    "\n",
    "# Create an empty column for region\n",
    "population_df['region'] = None\n",
    "\n",
    "# Function to detect if a row is a country by checking if the 'Geography' is NOT all uppercase\n",
    "def is_country(row):\n",
    "    return not row['Geography'].isupper()\n",
    "\n",
    "# Temporary variable to keep track of the current region\n",
    "current_region = None\n",
    "\n",
    "# Loop through the rows and update the region for each country\n",
    "for index, row in population_df.iterrows():\n",
    "    geography = row['Geography']\n",
    "    \n",
    "    # If it's not a country (i.e., the geography name is in all caps), it's a region\n",
    "    if not is_country(row):\n",
    "        current_region = geography  # Set the region\n",
    "    else:\n",
    "        # If it's a country, assign the current region to the 'region' column\n",
    "        population_df.at[index, 'region'] = current_region\n",
    "\n",
    "# Save the updated population DataFrame\n",
    "output_csv_path = \"intermediate-data/population_by_country_with_regions.csv\"\n",
    "population_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Updated population data with regions saved as {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Politicians and Population Data\n",
    "\n",
    "We merge the politician article data with the population data to perform country and regional-level analysis. The goal is to combine the Wikipedia article information with country population statistics and geographic region data, allowing us to analyze the distribution of articles and their quality across different countries and regions.\n",
    "\n",
    "Key steps:\n",
    "1. **Load the Data:**  \n",
    "   We load the previously processed `politicians_with_partial_prediction.csv` file, which contains Wikipedia article data, and the population dataset (with region and continent information).\n",
    "\n",
    "2. **Merging the DataFrames:**  \n",
    "   The two datasets are merged based on the country field. We use an outer join to ensure that countries missing from either dataset are retained for further investigation. The `_merge` indicator column helps us identify where matches failed.\n",
    "\n",
    "3. **Identifying Unmatched Countries:**  \n",
    "   Countries that exist in the Wikipedia dataset but not in the population data (and vice versa) are logged into a text file (`wp_countries-no_match.txt`). This helps us identify any inconsistencies between the datasets and handle them appropriately.\n",
    "\n",
    "4. **Filtering Matched Data:**  \n",
    "   We filter the merged DataFrame to retain only the rows where both the Wikipedia and population data matched. These matched rows are used for further analysis.\n",
    "\n",
    "5. **Renaming and Formatting Columns:**  \n",
    "   The columns are renamed and reformatted to meet the required format for the final output, which includes country, region, population, article title, revision ID, and article quality.\n",
    "\n",
    "This step ensures that our dataset is correctly structured for analysis by country and region, while also handling any inconsistencies between the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched countries from both datasets logged to wp_countries-no_match.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the politicians_with_partial_prediction.csv file\n",
    "politicians_df = pd.read_csv(\"intermediate-data/politicians_with_partial_prediction.csv\")\n",
    "\n",
    "# Load the population data file (already preprocessed with continent and region info)\n",
    "# population_df = pd.read_csv(\"raw-data/population_by_country_AUG.2024.csv\")\n",
    "\n",
    "# Merge the two DataFrames: 'country' from politicians_df and 'Geography' from population_df\n",
    "merged_df = pd.merge(\n",
    "    politicians_df[['country', 'name', 'lastrevid', 'prediction']],\n",
    "    population_df[['Geography', 'Population', 'region']],\n",
    "    left_on='country',\n",
    "    right_on='Geography',\n",
    "    how='outer',  # Use 'outer' to ensure we capture non-matching countries on both sides\n",
    "    indicator=True  # This will add a column to indicate where the match failed\n",
    ")\n",
    "\n",
    "# Separate unmatched countries from Wikipedia (left_only) and unmatched countries from Population (right_only)\n",
    "unmatched_wp_countries = merged_df[merged_df['_merge'] == 'left_only']['country'].unique()\n",
    "unmatched_population_countries = merged_df[merged_df['_merge'] == 'right_only']['Geography'].unique()\n",
    "\n",
    "# Write unmatched countries to wp_countries-no_match.txt\n",
    "log_file_path = \"wp_countries-no_match.txt\"\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    # Log unmatched countries from Wikipedia dataset\n",
    "    log_file.write(\"Unmatched Wikipedia Countries:\\n\")\n",
    "    for country in unmatched_wp_countries:\n",
    "        log_file.write(f\"{country}\\n\")\n",
    "\n",
    "    # Log unmatched countries from Population dataset\n",
    "    log_file.write(\"\\nUnmatched Population Countries:\\n\")\n",
    "    for country in unmatched_population_countries:\n",
    "        log_file.write(f\"{country}\\n\")\n",
    "\n",
    "print(f\"Unmatched countries from both datasets logged to {log_file_path}\")\n",
    "\n",
    "# Filter out only the matched rows (both in 'both') for final output\n",
    "matched_df = merged_df[merged_df['_merge'] == 'both']\n",
    "\n",
    "# Rename and reformat the columns to match the final CSV requirement\n",
    "matched_df = matched_df.rename(columns={\n",
    "    'name': 'article_title',\n",
    "    'lastrevid': 'revision_id',\n",
    "    'prediction': 'article_quality',\n",
    "    'Population': 'population'\n",
    "})\n",
    "\n",
    "# Select and reorder the required columns for the final output\n",
    "final_columns = ['country', 'region', 'population', 'article_title', 'revision_id', 'article_quality']\n",
    "final_df = matched_df[final_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Unmatched Countries and Applying Corrections\n",
    "\n",
    "We load the list of unmatched countries and apply corrections for known issues (e.g., punctuation differences). For each corrected country, we merge the corresponding population and region data into the final dataset. We then update the unmatched countries list by removing corrected entries and filtering out regions or continents. Finally, we log any remaining unmatched countries and save the updated dataset to `wp_politicians_by_country.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated unmatched countries logged to wp_countries-no_match.txt\n",
      "Updated dataframe saved as wp_politicians_by_country.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the unmatched countries file (initial version)\n",
    "with open(\"wp_countries-no_match.txt\", \"r\") as log_file:\n",
    "    unmatched_countries = log_file.read().splitlines()\n",
    "\n",
    "# Known punctuation issues to be corrected\n",
    "corrections = {\n",
    "    \"Guinea-Bissau\": \"GuineaBissau\",\n",
    "    \"Korea, South\": \"Korea (South)\"\n",
    "}\n",
    "\n",
    "# 1. For each correction, find the row in the population_df and append it to the matched_df\n",
    "for incorrect, correct in corrections.items():\n",
    "    # Get the corresponding population data from population_df\n",
    "    corrected_row = politicians_df[politicians_df['country'] == incorrect].copy()\n",
    "    population_data = population_df[population_df['Geography'] == correct].copy()\n",
    "    \n",
    "    # If a match is found, add the corrected population and region to the corrected_row\n",
    "    if not population_data.empty:\n",
    "        corrected_row['population'] = population_data['Population'].values[0]\n",
    "        corrected_row['region'] = population_data['region'].values[0]\n",
    "        \n",
    "        corrected_row = corrected_row.drop(columns=['url'])\n",
    "        # Rename columns to match the final format\n",
    "        corrected_row = corrected_row.rename(columns={\n",
    "            'name': 'article_title',\n",
    "            'lastrevid': 'revision_id',\n",
    "            'prediction': 'article_quality'\n",
    "        })\n",
    "\n",
    "        \n",
    "        # Append the corrected row back to matched_df\n",
    "        final_df = pd.concat([final_df, corrected_row])\n",
    "\n",
    "\n",
    "# 2. Remove the corrected countries from the unmatched list\n",
    "unmatched_countries = [country for country in unmatched_countries if country not in corrections.keys()]\n",
    "# Remove corrected country names from the unmatched list\n",
    "unmatched_countries = [country for country in unmatched_countries if country not in corrections.values()]\n",
    "\n",
    "# 3. Remove all-uppercase rows (regions/continents) from the unmatched list\n",
    "def is_all_uppercase(name):\n",
    "    return name.isupper()\n",
    "\n",
    "unmatched_countries = [country for country in unmatched_countries if not is_all_uppercase(country)]\n",
    "\n",
    "# Append the updated unmatched countries to the same log file\n",
    "log_file_path = \"wp_countries-no_match.txt\"\n",
    "with open(log_file_path, \"w\") as log_file:  # Change \"a\" to \"w\" to overwrite the existing file\n",
    "    for country in unmatched_countries:\n",
    "        log_file.write(f\"{country}\\n\")\n",
    "\n",
    "print(f\"Updated unmatched countries logged to {log_file_path}\")\n",
    "\n",
    "# Save the updated final dataset with corrections applied\n",
    "final_df.to_csv(\"wp_politicians_by_country.csv\", index=False)\n",
    "\n",
    "print(f\"Updated dataframe saved as wp_politicians_by_country.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Articles per Capita and High-Quality Articles\n",
    "\n",
    "We calculate the total articles and high-quality articles (FA, GA) per capita for each country. First, we group the data by country to count the total articles and population. Then, we calculate the number of articles per million people. We also filter for high-quality articles, calculate the high-quality articles per capita, and merge these results into the country-level data for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_articles</th>\n",
       "      <th>total_population</th>\n",
       "      <th>articles_per_mil_capita</th>\n",
       "      <th>high_quality_articles</th>\n",
       "      <th>high_quality_articles_per_mil_capita</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>85</td>\n",
       "      <td>42.4</td>\n",
       "      <td>2.004717</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.070755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>70</td>\n",
       "      <td>2.7</td>\n",
       "      <td>25.925926</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>71</td>\n",
       "      <td>46.8</td>\n",
       "      <td>1.517094</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <td>58</td>\n",
       "      <td>36.7</td>\n",
       "      <td>1.580381</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.054496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antigua and Barbuda</th>\n",
       "      <td>33</td>\n",
       "      <td>0.1</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     total_articles  total_population  \\\n",
       "country                                                 \n",
       "Afghanistan                      85              42.4   \n",
       "Albania                          70               2.7   \n",
       "Algeria                          71              46.8   \n",
       "Angola                           58              36.7   \n",
       "Antigua and Barbuda              33               0.1   \n",
       "\n",
       "                     articles_per_mil_capita  high_quality_articles  \\\n",
       "country                                                               \n",
       "Afghanistan                         2.004717                    3.0   \n",
       "Albania                            25.925926                    7.0   \n",
       "Algeria                             1.517094                    1.0   \n",
       "Angola                              1.580381                    2.0   \n",
       "Antigua and Barbuda               330.000000                    0.0   \n",
       "\n",
       "                     high_quality_articles_per_mil_capita  \n",
       "country                                                    \n",
       "Afghanistan                                      0.070755  \n",
       "Albania                                          2.592593  \n",
       "Algeria                                          0.021368  \n",
       "Angola                                           0.054496  \n",
       "Antigua and Barbuda                              0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the final dataset\n",
    "final_df = pd.read_csv(\"wp_politicians_by_country.csv\")\n",
    "\n",
    "# Group by country to get the total number of articles and population per country\n",
    "country_grouped = final_df.groupby('country').agg(\n",
    "    total_articles=('article_title', 'count'),\n",
    "    total_population=('population', 'first')\n",
    ")\n",
    "\n",
    "# Calculate total articles per capita for each country\n",
    "country_grouped['articles_per_mil_capita'] = country_grouped['total_articles'] / country_grouped['total_population']\n",
    "\n",
    "# Filter high-quality articles (FA, GA)\n",
    "high_quality_articles = final_df[final_df['article_quality'].isin(['FA', 'GA'])]\n",
    "\n",
    "# Group by country to get high-quality articles count\n",
    "high_quality_grouped = high_quality_articles.groupby('country').agg(\n",
    "    high_quality_articles=('article_title', 'count')\n",
    ")\n",
    "\n",
    "# Merge high-quality articles count into the country-level data\n",
    "country_grouped = country_grouped.merge(high_quality_grouped, on='country', how='left').fillna(0)\n",
    "\n",
    "# Calculate high-quality articles per capita\n",
    "country_grouped['high_quality_articles_per_mil_capita'] = country_grouped['high_quality_articles'] / country_grouped['total_population']\n",
    "\n",
    "# Show the calculated values\n",
    "country_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Total Articles and High-Quality Articles Per Capita (Regional-Level)\n",
    "\n",
    "This step calculates the total and high-quality articles per capita at the regional level. We group the data by region to sum the total articles and population, then compute the articles per million people. High-quality articles are also grouped by region, and the results are merged to calculate high-quality articles per capita for each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_articles</th>\n",
       "      <th>total_population</th>\n",
       "      <th>articles_per_mil_capita</th>\n",
       "      <th>high_quality_articles</th>\n",
       "      <th>high_quality_articles_per_mil_capita</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CARIBBEAN</th>\n",
       "      <td>219</td>\n",
       "      <td>1414.9</td>\n",
       "      <td>0.154781</td>\n",
       "      <td>9</td>\n",
       "      <td>0.006361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CENTRAL AMERICA</th>\n",
       "      <td>188</td>\n",
       "      <td>1418.8</td>\n",
       "      <td>0.132506</td>\n",
       "      <td>10</td>\n",
       "      <td>0.007048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CENTRAL ASIA</th>\n",
       "      <td>106</td>\n",
       "      <td>1983.6</td>\n",
       "      <td>0.053438</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EAST ASIA</th>\n",
       "      <td>230</td>\n",
       "      <td>41422.0</td>\n",
       "      <td>0.005553</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EASTERN AFRICA</th>\n",
       "      <td>665</td>\n",
       "      <td>23941.2</td>\n",
       "      <td>0.027776</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 total_articles  total_population  articles_per_mil_capita  \\\n",
       "region                                                                       \n",
       "CARIBBEAN                   219            1414.9                 0.154781   \n",
       "CENTRAL AMERICA             188            1418.8                 0.132506   \n",
       "CENTRAL ASIA                106            1983.6                 0.053438   \n",
       "EAST ASIA                   230           41422.0                 0.005553   \n",
       "EASTERN AFRICA              665           23941.2                 0.027776   \n",
       "\n",
       "                 high_quality_articles  high_quality_articles_per_mil_capita  \n",
       "region                                                                        \n",
       "CARIBBEAN                            9                              0.006361  \n",
       "CENTRAL AMERICA                     10                              0.007048  \n",
       "CENTRAL ASIA                         5                              0.002521  \n",
       "EAST ASIA                           13                              0.000314  \n",
       "EASTERN AFRICA                      17                              0.000710  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by region for regional totals\n",
    "region_grouped = final_df.groupby('region').agg(\n",
    "    total_articles=('article_title', 'count'),\n",
    "    total_population=('population', 'sum')\n",
    ")\n",
    "\n",
    "# Calculate total articles per capita for each region\n",
    "region_grouped['articles_per_mil_capita'] = region_grouped['total_articles'] / region_grouped['total_population']\n",
    "\n",
    "# Group high-quality articles by region\n",
    "high_quality_region_grouped = high_quality_articles.groupby('region').agg(\n",
    "    high_quality_articles=('article_title', 'count')\n",
    ")\n",
    "\n",
    "# Merge with the regional data\n",
    "region_grouped = region_grouped.merge(high_quality_region_grouped, on='region', how='left').fillna(0)\n",
    "\n",
    "# Calculate high-quality articles per capita for each region\n",
    "region_grouped['high_quality_articles_per_mil_capita'] = region_grouped['high_quality_articles'] / region_grouped['total_population']\n",
    "\n",
    "# Show the calculated values\n",
    "region_grouped.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result 1: Top 10 Countries by Coverage (Articles Per Capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_44545\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_44545_level0_col0\" class=\"col_heading level0 col0\" >total_articles</th>\n",
       "      <th id=\"T_44545_level0_col1\" class=\"col_heading level0 col1\" >articles_per_mil_capita</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >country</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_44545_level0_row0\" class=\"row_heading level0 row0\" >Monaco</th>\n",
       "      <td id=\"T_44545_row0_col0\" class=\"data row0 col0\" >10</td>\n",
       "      <td id=\"T_44545_row0_col1\" class=\"data row0 col1\" >inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44545_level0_row1\" class=\"row_heading level0 row1\" >Tuvalu</th>\n",
       "      <td id=\"T_44545_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_44545_row1_col1\" class=\"data row1 col1\" >inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44545_level0_row2\" class=\"row_heading level0 row2\" >Antigua and Barbuda</th>\n",
       "      <td id=\"T_44545_row2_col0\" class=\"data row2 col0\" >33</td>\n",
       "      <td id=\"T_44545_row2_col1\" class=\"data row2 col1\" >330.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44545_level0_row3\" class=\"row_heading level0 row3\" >Federated States of Micronesia</th>\n",
       "      <td id=\"T_44545_row3_col0\" class=\"data row3 col0\" >14</td>\n",
       "      <td id=\"T_44545_row3_col1\" class=\"data row3 col1\" >140.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44545_level0_row4\" class=\"row_heading level0 row4\" >Marshall Islands</th>\n",
       "      <td id=\"T_44545_row4_col0\" class=\"data row4 col0\" >13</td>\n",
       "      <td id=\"T_44545_row4_col1\" class=\"data row4 col1\" >130.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44545_level0_row5\" class=\"row_heading level0 row5\" >Tonga</th>\n",
       "      <td id=\"T_44545_row5_col0\" class=\"data row5 col0\" >10</td>\n",
       "      <td id=\"T_44545_row5_col1\" class=\"data row5 col1\" >100.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44545_level0_row6\" class=\"row_heading level0 row6\" >Barbados</th>\n",
       "      <td id=\"T_44545_row6_col0\" class=\"data row6 col0\" >25</td>\n",
       "      <td id=\"T_44545_row6_col1\" class=\"data row6 col1\" >83.3333333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44545_level0_row7\" class=\"row_heading level0 row7\" >Seychelles</th>\n",
       "      <td id=\"T_44545_row7_col0\" class=\"data row7 col0\" >6</td>\n",
       "      <td id=\"T_44545_row7_col1\" class=\"data row7 col1\" >60.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44545_level0_row8\" class=\"row_heading level0 row8\" >Montenegro</th>\n",
       "      <td id=\"T_44545_row8_col0\" class=\"data row8 col0\" >36</td>\n",
       "      <td id=\"T_44545_row8_col1\" class=\"data row8 col1\" >60.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44545_level0_row9\" class=\"row_heading level0 row9\" >Maldives</th>\n",
       "      <td id=\"T_44545_row9_col0\" class=\"data row9 col0\" >33</td>\n",
       "      <td id=\"T_44545_row9_col1\" class=\"data row9 col1\" >55.0000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16cd78550>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 countries by total articles per capita\n",
    "top_10_countries_by_coverage = country_grouped.sort_values('articles_per_mil_capita', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 countries\n",
    "top_10_countries_by_coverage[['total_articles', 'articles_per_mil_capita']].style.format({'articles_per_mil_capita': '{:.10f}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result 2: Bottom 10 Countries by Coverage (Articles Per Capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_fd8bd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fd8bd_level0_col0\" class=\"col_heading level0 col0\" >total_articles</th>\n",
       "      <th id=\"T_fd8bd_level0_col1\" class=\"col_heading level0 col1\" >articles_per_mil_capita</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >country</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fd8bd_level0_row0\" class=\"row_heading level0 row0\" >China</th>\n",
       "      <td id=\"T_fd8bd_row0_col0\" class=\"data row0 col0\" >16</td>\n",
       "      <td id=\"T_fd8bd_row0_col1\" class=\"data row0 col1\" >0.0113370651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd8bd_level0_row1\" class=\"row_heading level0 row1\" >India</th>\n",
       "      <td id=\"T_fd8bd_row1_col0\" class=\"data row1 col0\" >151</td>\n",
       "      <td id=\"T_fd8bd_row1_col1\" class=\"data row1 col1\" >0.1056978860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd8bd_level0_row2\" class=\"row_heading level0 row2\" >Ghana</th>\n",
       "      <td id=\"T_fd8bd_row2_col0\" class=\"data row2 col0\" >4</td>\n",
       "      <td id=\"T_fd8bd_row2_col1\" class=\"data row2 col1\" >0.1173020528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd8bd_level0_row3\" class=\"row_heading level0 row3\" >Saudi Arabia</th>\n",
       "      <td id=\"T_fd8bd_row3_col0\" class=\"data row3 col0\" >5</td>\n",
       "      <td id=\"T_fd8bd_row3_col1\" class=\"data row3 col1\" >0.1355013550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd8bd_level0_row4\" class=\"row_heading level0 row4\" >Zambia</th>\n",
       "      <td id=\"T_fd8bd_row4_col0\" class=\"data row4 col0\" >3</td>\n",
       "      <td id=\"T_fd8bd_row4_col1\" class=\"data row4 col1\" >0.1485148515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd8bd_level0_row5\" class=\"row_heading level0 row5\" >Norway</th>\n",
       "      <td id=\"T_fd8bd_row5_col0\" class=\"data row5 col0\" >1</td>\n",
       "      <td id=\"T_fd8bd_row5_col1\" class=\"data row5 col1\" >0.1818181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd8bd_level0_row6\" class=\"row_heading level0 row6\" >Israel</th>\n",
       "      <td id=\"T_fd8bd_row6_col0\" class=\"data row6 col0\" >2</td>\n",
       "      <td id=\"T_fd8bd_row6_col1\" class=\"data row6 col1\" >0.2040816327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd8bd_level0_row7\" class=\"row_heading level0 row7\" >Egypt</th>\n",
       "      <td id=\"T_fd8bd_row7_col0\" class=\"data row7 col0\" >32</td>\n",
       "      <td id=\"T_fd8bd_row7_col1\" class=\"data row7 col1\" >0.3041825095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd8bd_level0_row8\" class=\"row_heading level0 row8\" >Cote d'Ivoire</th>\n",
       "      <td id=\"T_fd8bd_row8_col0\" class=\"data row8 col0\" >10</td>\n",
       "      <td id=\"T_fd8bd_row8_col1\" class=\"data row8 col1\" >0.3236245955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd8bd_level0_row9\" class=\"row_heading level0 row9\" >Ethiopia</th>\n",
       "      <td id=\"T_fd8bd_row9_col0\" class=\"data row9 col0\" >44</td>\n",
       "      <td id=\"T_fd8bd_row9_col1\" class=\"data row9 col1\" >0.3478260870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16f6dcc90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bottom 10 countries by total articles per capita\n",
    "bottom_10_countries_by_coverage = country_grouped.sort_values('articles_per_mil_capita', ascending=True).head(10)\n",
    "\n",
    "# Display the bottom 10 countries\n",
    "bottom_10_countries_by_coverage[['total_articles', 'articles_per_mil_capita']].style.format({'articles_per_mil_capita': '{:.10f}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result 3: Top 10 Countries by High-Quality Articles Per Capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1b8bb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1b8bb_level0_col0\" class=\"col_heading level0 col0\" >high_quality_articles</th>\n",
       "      <th id=\"T_1b8bb_level0_col1\" class=\"col_heading level0 col1\" >high_quality_articles_per_mil_capita</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >country</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1b8bb_level0_row0\" class=\"row_heading level0 row0\" >Montenegro</th>\n",
       "      <td id=\"T_1b8bb_row0_col0\" class=\"data row0 col0\" >3.000000</td>\n",
       "      <td id=\"T_1b8bb_row0_col1\" class=\"data row0 col1\" >5.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b8bb_level0_row1\" class=\"row_heading level0 row1\" >Luxembourg</th>\n",
       "      <td id=\"T_1b8bb_row1_col0\" class=\"data row1 col0\" >2.000000</td>\n",
       "      <td id=\"T_1b8bb_row1_col1\" class=\"data row1 col1\" >2.8571428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b8bb_level0_row2\" class=\"row_heading level0 row2\" >Albania</th>\n",
       "      <td id=\"T_1b8bb_row2_col0\" class=\"data row2 col0\" >7.000000</td>\n",
       "      <td id=\"T_1b8bb_row2_col1\" class=\"data row2 col1\" >2.5925925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b8bb_level0_row3\" class=\"row_heading level0 row3\" >Kosovo</th>\n",
       "      <td id=\"T_1b8bb_row3_col0\" class=\"data row3 col0\" >4.000000</td>\n",
       "      <td id=\"T_1b8bb_row3_col1\" class=\"data row3 col1\" >2.3529411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b8bb_level0_row4\" class=\"row_heading level0 row4\" >Maldives</th>\n",
       "      <td id=\"T_1b8bb_row4_col0\" class=\"data row4 col0\" >1.000000</td>\n",
       "      <td id=\"T_1b8bb_row4_col1\" class=\"data row4 col1\" >1.6666666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b8bb_level0_row5\" class=\"row_heading level0 row5\" >Lithuania</th>\n",
       "      <td id=\"T_1b8bb_row5_col0\" class=\"data row5 col0\" >4.000000</td>\n",
       "      <td id=\"T_1b8bb_row5_col1\" class=\"data row5 col1\" >1.3793103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b8bb_level0_row6\" class=\"row_heading level0 row6\" >Croatia</th>\n",
       "      <td id=\"T_1b8bb_row6_col0\" class=\"data row6 col0\" >5.000000</td>\n",
       "      <td id=\"T_1b8bb_row6_col1\" class=\"data row6 col1\" >1.3157894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b8bb_level0_row7\" class=\"row_heading level0 row7\" >Guyana</th>\n",
       "      <td id=\"T_1b8bb_row7_col0\" class=\"data row7 col0\" >1.000000</td>\n",
       "      <td id=\"T_1b8bb_row7_col1\" class=\"data row7 col1\" >1.2500000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b8bb_level0_row8\" class=\"row_heading level0 row8\" >Palestinian Territory</th>\n",
       "      <td id=\"T_1b8bb_row8_col0\" class=\"data row8 col0\" >6.000000</td>\n",
       "      <td id=\"T_1b8bb_row8_col1\" class=\"data row8 col1\" >1.0909090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1b8bb_level0_row9\" class=\"row_heading level0 row9\" >Slovenia</th>\n",
       "      <td id=\"T_1b8bb_row9_col0\" class=\"data row9 col0\" >2.000000</td>\n",
       "      <td id=\"T_1b8bb_row9_col1\" class=\"data row9 col1\" >0.9523809524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16f93acd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 countries by high-quality articles per capita\n",
    "top_10_countries_by_high_quality = country_grouped.sort_values('high_quality_articles_per_mil_capita', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 countries\n",
    "top_10_countries_by_high_quality[['high_quality_articles', 'high_quality_articles_per_mil_capita']].style.format({'high_quality_articles_per_mil_capita': '{:.10f}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result 4: Bottom 10 Countries by High-Quality Articles Per Capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1d750\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1d750_level0_col0\" class=\"col_heading level0 col0\" >high_quality_articles</th>\n",
       "      <th id=\"T_1d750_level0_col1\" class=\"col_heading level0 col1\" >high_quality_articles_per_mil_capita</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >country</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1d750_level0_row0\" class=\"row_heading level0 row0\" >Zimbabwe</th>\n",
       "      <td id=\"T_1d750_row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
       "      <td id=\"T_1d750_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d750_level0_row1\" class=\"row_heading level0 row1\" >Qatar</th>\n",
       "      <td id=\"T_1d750_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
       "      <td id=\"T_1d750_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d750_level0_row2\" class=\"row_heading level0 row2\" >Grenada</th>\n",
       "      <td id=\"T_1d750_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
       "      <td id=\"T_1d750_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d750_level0_row3\" class=\"row_heading level0 row3\" >Gambia</th>\n",
       "      <td id=\"T_1d750_row3_col0\" class=\"data row3 col0\" >0.000000</td>\n",
       "      <td id=\"T_1d750_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d750_level0_row4\" class=\"row_heading level0 row4\" >Samoa</th>\n",
       "      <td id=\"T_1d750_row4_col0\" class=\"data row4 col0\" >0.000000</td>\n",
       "      <td id=\"T_1d750_row4_col1\" class=\"data row4 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d750_level0_row5\" class=\"row_heading level0 row5\" >Senegal</th>\n",
       "      <td id=\"T_1d750_row5_col0\" class=\"data row5 col0\" >0.000000</td>\n",
       "      <td id=\"T_1d750_row5_col1\" class=\"data row5 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d750_level0_row6\" class=\"row_heading level0 row6\" >Federated States of Micronesia</th>\n",
       "      <td id=\"T_1d750_row6_col0\" class=\"data row6 col0\" >0.000000</td>\n",
       "      <td id=\"T_1d750_row6_col1\" class=\"data row6 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d750_level0_row7\" class=\"row_heading level0 row7\" >Estonia</th>\n",
       "      <td id=\"T_1d750_row7_col0\" class=\"data row7 col0\" >0.000000</td>\n",
       "      <td id=\"T_1d750_row7_col1\" class=\"data row7 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d750_level0_row8\" class=\"row_heading level0 row8\" >Eritrea</th>\n",
       "      <td id=\"T_1d750_row8_col0\" class=\"data row8 col0\" >0.000000</td>\n",
       "      <td id=\"T_1d750_row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d750_level0_row9\" class=\"row_heading level0 row9\" >Equatorial Guinea</th>\n",
       "      <td id=\"T_1d750_row9_col0\" class=\"data row9 col0\" >0.000000</td>\n",
       "      <td id=\"T_1d750_row9_col1\" class=\"data row9 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16f975590>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bottom 10 countries by high-quality articles per capita\n",
    "bottom_10_countries_by_high_quality = country_grouped.sort_values('high_quality_articles_per_mil_capita', ascending=True).head(10)\n",
    "\n",
    "# Display the bottom 10 countries\n",
    "bottom_10_countries_by_high_quality[['high_quality_articles', 'high_quality_articles_per_mil_capita']].style.format({'mil_high_quality_articles_per_capita': '{:.10f}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result 5: Geographic Regions by Total Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_262fd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_262fd_level0_col0\" class=\"col_heading level0 col0\" >total_articles</th>\n",
       "      <th id=\"T_262fd_level0_col1\" class=\"col_heading level0 col1\" >articles_per_mil_capita</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >region</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row0\" class=\"row_heading level0 row0\" >OCEANIA</th>\n",
       "      <td id=\"T_262fd_row0_col0\" class=\"data row0 col0\" >72</td>\n",
       "      <td id=\"T_262fd_row0_col1\" class=\"data row0 col1\" >0.6480648065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row1\" class=\"row_heading level0 row1\" >NORTHERN EUROPE</th>\n",
       "      <td id=\"T_262fd_row1_col0\" class=\"data row1 col0\" >191</td>\n",
       "      <td id=\"T_262fd_row1_col1\" class=\"data row1 col1\" >0.1643576284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row2\" class=\"row_heading level0 row2\" >CARIBBEAN</th>\n",
       "      <td id=\"T_262fd_row2_col0\" class=\"data row2 col0\" >219</td>\n",
       "      <td id=\"T_262fd_row2_col1\" class=\"data row2 col1\" >0.1547812566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row3\" class=\"row_heading level0 row3\" >CENTRAL AMERICA</th>\n",
       "      <td id=\"T_262fd_row3_col0\" class=\"data row3 col0\" >188</td>\n",
       "      <td id=\"T_262fd_row3_col1\" class=\"data row3 col1\" >0.1325063434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row4\" class=\"row_heading level0 row4\" >CENTRAL ASIA</th>\n",
       "      <td id=\"T_262fd_row4_col0\" class=\"data row4 col0\" >106</td>\n",
       "      <td id=\"T_262fd_row4_col1\" class=\"data row4 col1\" >0.0534381932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row5\" class=\"row_heading level0 row5\" >WESTERN ASIA</th>\n",
       "      <td id=\"T_262fd_row5_col0\" class=\"data row5 col0\" >610</td>\n",
       "      <td id=\"T_262fd_row5_col1\" class=\"data row5 col1\" >0.0456262388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row6\" class=\"row_heading level0 row6\" >SOUTHERN EUROPE</th>\n",
       "      <td id=\"T_262fd_row6_col0\" class=\"data row6 col0\" >797</td>\n",
       "      <td id=\"T_262fd_row6_col1\" class=\"data row6 col1\" >0.0443847944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row7\" class=\"row_heading level0 row7\" >EASTERN AFRICA</th>\n",
       "      <td id=\"T_262fd_row7_col0\" class=\"data row7 col0\" >665</td>\n",
       "      <td id=\"T_262fd_row7_col1\" class=\"data row7 col1\" >0.0277763855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row8\" class=\"row_heading level0 row8\" >WESTERN EUROPE</th>\n",
       "      <td id=\"T_262fd_row8_col0\" class=\"data row8 col0\" >498</td>\n",
       "      <td id=\"T_262fd_row8_col1\" class=\"data row8 col1\" >0.0262521152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row9\" class=\"row_heading level0 row9\" >NORTHERN AFRICA</th>\n",
       "      <td id=\"T_262fd_row9_col0\" class=\"data row9 col0\" >302</td>\n",
       "      <td id=\"T_262fd_row9_col1\" class=\"data row9 col1\" >0.0248071694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row10\" class=\"row_heading level0 row10\" >EASTERN EUROPE</th>\n",
       "      <td id=\"T_262fd_row10_col0\" class=\"data row10 col0\" >709</td>\n",
       "      <td id=\"T_262fd_row10_col1\" class=\"data row10 col1\" >0.0244104817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row11\" class=\"row_heading level0 row11\" >MIDDLE AFRICA</th>\n",
       "      <td id=\"T_262fd_row11_col0\" class=\"data row11 col0\" >231</td>\n",
       "      <td id=\"T_262fd_row11_col1\" class=\"data row11 col1\" >0.0242522231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row12\" class=\"row_heading level0 row12\" >SOUTHERN AFRICA</th>\n",
       "      <td id=\"T_262fd_row12_col0\" class=\"data row12 col0\" >123</td>\n",
       "      <td id=\"T_262fd_row12_col1\" class=\"data row12 col1\" >0.0206573401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row13\" class=\"row_heading level0 row13\" >SOUTH AMERICA</th>\n",
       "      <td id=\"T_262fd_row13_col0\" class=\"data row13 col0\" >569</td>\n",
       "      <td id=\"T_262fd_row13_col1\" class=\"data row13 col1\" >0.0164860159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row14\" class=\"row_heading level0 row14\" >WESTERN AFRICA</th>\n",
       "      <td id=\"T_262fd_row14_col0\" class=\"data row14 col0\" >523</td>\n",
       "      <td id=\"T_262fd_row14_col1\" class=\"data row14 col1\" >0.0088644519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row15\" class=\"row_heading level0 row15\" >SOUTHEAST ASIA</th>\n",
       "      <td id=\"T_262fd_row15_col0\" class=\"data row15 col0\" >396</td>\n",
       "      <td id=\"T_262fd_row15_col1\" class=\"data row15 col1\" >0.0087944497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row16\" class=\"row_heading level0 row16\" >EAST ASIA</th>\n",
       "      <td id=\"T_262fd_row16_col0\" class=\"data row16 col0\" >230</td>\n",
       "      <td id=\"T_262fd_row16_col1\" class=\"data row16 col1\" >0.0055526049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_262fd_level0_row17\" class=\"row_heading level0 row17\" >SOUTH ASIA</th>\n",
       "      <td id=\"T_262fd_row17_col0\" class=\"data row17 col0\" >670</td>\n",
       "      <td id=\"T_262fd_row17_col1\" class=\"data row17 col1\" >0.0025363262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16f942310>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Geographic regions by total articles per capita\n",
    "regions_by_coverage = region_grouped.sort_values('articles_per_mil_capita', ascending=False)\n",
    "\n",
    "# Display the regions ranked by total coverage\n",
    "regions_by_coverage[['total_articles', 'articles_per_mil_capita']].style.format({'articles_per_mil_capita': '{:.10f}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result 6: Geographic Regions by High-Quality Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_23c7e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_23c7e_level0_col0\" class=\"col_heading level0 col0\" >high_quality_articles</th>\n",
       "      <th id=\"T_23c7e_level0_col1\" class=\"col_heading level0 col1\" >high_quality_articles_per_mil_capita</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >region</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row0\" class=\"row_heading level0 row0\" >OCEANIA</th>\n",
       "      <td id=\"T_23c7e_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_23c7e_row0_col1\" class=\"data row0 col1\" >0.0090009001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row1\" class=\"row_heading level0 row1\" >NORTHERN EUROPE</th>\n",
       "      <td id=\"T_23c7e_row1_col0\" class=\"data row1 col0\" >9</td>\n",
       "      <td id=\"T_23c7e_row1_col1\" class=\"data row1 col1\" >0.0077446003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row2\" class=\"row_heading level0 row2\" >CENTRAL AMERICA</th>\n",
       "      <td id=\"T_23c7e_row2_col0\" class=\"data row2 col0\" >10</td>\n",
       "      <td id=\"T_23c7e_row2_col1\" class=\"data row2 col1\" >0.0070482098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row3\" class=\"row_heading level0 row3\" >CARIBBEAN</th>\n",
       "      <td id=\"T_23c7e_row3_col0\" class=\"data row3 col0\" >9</td>\n",
       "      <td id=\"T_23c7e_row3_col1\" class=\"data row3 col1\" >0.0063608736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row4\" class=\"row_heading level0 row4\" >SOUTHERN EUROPE</th>\n",
       "      <td id=\"T_23c7e_row4_col0\" class=\"data row4 col0\" >53</td>\n",
       "      <td id=\"T_23c7e_row4_col1\" class=\"data row4 col1\" >0.0029515610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row5\" class=\"row_heading level0 row5\" >CENTRAL ASIA</th>\n",
       "      <td id=\"T_23c7e_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "      <td id=\"T_23c7e_row5_col1\" class=\"data row5 col1\" >0.0025206695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row6\" class=\"row_heading level0 row6\" >WESTERN ASIA</th>\n",
       "      <td id=\"T_23c7e_row6_col0\" class=\"data row6 col0\" >27</td>\n",
       "      <td id=\"T_23c7e_row6_col1\" class=\"data row6 col1\" >0.0020195220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row7\" class=\"row_heading level0 row7\" >NORTHERN AFRICA</th>\n",
       "      <td id=\"T_23c7e_row7_col0\" class=\"data row7 col0\" >17</td>\n",
       "      <td id=\"T_23c7e_row7_col1\" class=\"data row7 col1\" >0.0013964301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row8\" class=\"row_heading level0 row8\" >SOUTHERN AFRICA</th>\n",
       "      <td id=\"T_23c7e_row8_col0\" class=\"data row8 col0\" >8</td>\n",
       "      <td id=\"T_23c7e_row8_col1\" class=\"data row8 col1\" >0.0013435668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row9\" class=\"row_heading level0 row9\" >EASTERN EUROPE</th>\n",
       "      <td id=\"T_23c7e_row9_col0\" class=\"data row9 col0\" >38</td>\n",
       "      <td id=\"T_23c7e_row9_col1\" class=\"data row9 col1\" >0.0013083192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row10\" class=\"row_heading level0 row10\" >WESTERN EUROPE</th>\n",
       "      <td id=\"T_23c7e_row10_col0\" class=\"data row10 col0\" >21</td>\n",
       "      <td id=\"T_23c7e_row10_col1\" class=\"data row10 col1\" >0.0011070169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row11\" class=\"row_heading level0 row11\" >MIDDLE AFRICA</th>\n",
       "      <td id=\"T_23c7e_row11_col0\" class=\"data row11 col0\" >8</td>\n",
       "      <td id=\"T_23c7e_row11_col1\" class=\"data row11 col1\" >0.0008399038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row12\" class=\"row_heading level0 row12\" >EASTERN AFRICA</th>\n",
       "      <td id=\"T_23c7e_row12_col0\" class=\"data row12 col0\" >17</td>\n",
       "      <td id=\"T_23c7e_row12_col1\" class=\"data row12 col1\" >0.0007100730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row13\" class=\"row_heading level0 row13\" >SOUTHEAST ASIA</th>\n",
       "      <td id=\"T_23c7e_row13_col0\" class=\"data row13 col0\" >25</td>\n",
       "      <td id=\"T_23c7e_row13_col1\" class=\"data row13 col1\" >0.0005552052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row14\" class=\"row_heading level0 row14\" >SOUTH AMERICA</th>\n",
       "      <td id=\"T_23c7e_row14_col0\" class=\"data row14 col0\" >19</td>\n",
       "      <td id=\"T_23c7e_row14_col1\" class=\"data row14 col1\" >0.0005504997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row15\" class=\"row_heading level0 row15\" >EAST ASIA</th>\n",
       "      <td id=\"T_23c7e_row15_col0\" class=\"data row15 col0\" >13</td>\n",
       "      <td id=\"T_23c7e_row15_col1\" class=\"data row15 col1\" >0.0003138429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row16\" class=\"row_heading level0 row16\" >WESTERN AFRICA</th>\n",
       "      <td id=\"T_23c7e_row16_col0\" class=\"data row16 col0\" >13</td>\n",
       "      <td id=\"T_23c7e_row16_col1\" class=\"data row16 col1\" >0.0002203401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23c7e_level0_row17\" class=\"row_heading level0 row17\" >SOUTH ASIA</th>\n",
       "      <td id=\"T_23c7e_row17_col0\" class=\"data row17 col0\" >21</td>\n",
       "      <td id=\"T_23c7e_row17_col1\" class=\"data row17 col1\" >0.0000794968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16f6e9750>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Geographic regions by high-quality articles per capita\n",
    "regions_by_high_quality_coverage = region_grouped.sort_values('high_quality_articles_per_mil_capita', ascending=False)\n",
    "\n",
    "# Display the regions ranked by high-quality coverage\n",
    "regions_by_high_quality_coverage[['high_quality_articles', 'high_quality_articles_per_mil_capita']].style.format({'high_quality_articles_per_mil_capita': '{:.10f}'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLTutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
